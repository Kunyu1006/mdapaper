{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukun/Desktop/mda/mdatopic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. 读取所有 txt 文件 ==========\n",
    "root_path = \"/Users/yukun/Desktop/mda/MDA训练集\"\n",
    "file_list = sorted([f for f in os.listdir(root_path) if f.endswith('.txt')])\n",
    "documents = []\n",
    "\n",
    "for fname in file_list:\n",
    "    with open(os.path.join(root_path, fname), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        documents.append((fname, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. 切句 ==========\n",
    "def split_sentences(text):\n",
    "    return [s.strip() for s in re.split(r\"[。！？\\n]\", text) if len(s.strip()) > 10]\n",
    "\n",
    "all_sentences = []\n",
    "doc_indices = []\n",
    "\n",
    "for idx, (fname, text) in enumerate(documents):\n",
    "    sents = split_sentences(text)\n",
    "    all_sentences.extend(sents)\n",
    "    doc_indices.extend([idx] * len(sents))  # 记录句子属于哪个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. 加载 HuggingFace pipeline 作为 embedding ==========\n",
    "from transformers import pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class HFTransformerEmbedding(BaseEstimator):\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        embeddings = []\n",
    "        for doc in tqdm(documents, desc=\"Embedding Sentences\"):\n",
    "            features = self.pipe(doc, truncation=True, padding=True)\n",
    "            if isinstance(features, list):\n",
    "                features = np.mean(features[0], axis=0)\n",
    "            embeddings.append(features)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-zh\")\n",
    "embedding_model = HFTransformerEmbedding(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. 建立 BERTopic 模型 ==========\n",
    "topic_model = BERTopic(embedding_model=embedding_model, language=\"chinese\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 5. 每篇文档提取主题相关句子 ==========\n",
    "num_topics = min(10, len(set(topics)) - 1)  # -1是无关类\n",
    "docs = [[] for _ in range(len(documents))]  # 每个文档一组句子列表\n",
    "\n",
    "for tid in range(num_topics):\n",
    "    for sent, topic_id, doc_id in zip(all_sentences, topics, doc_indices):\n",
    "        if topic_id == tid:\n",
    "            docs[doc_id].append((tid, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 6. 构造结构化输出 DataFrame ==========\n",
    "result = pd.DataFrame()\n",
    "result[\"文件名\"] = [x[0] for x in documents]\n",
    "result[\"原文摘要\"] = [x[1][:300] for x in documents]  # 原文截断显示\n",
    "\n",
    "# 每个主题输出一列\n",
    "for tid in range(num_topics):\n",
    "    topic_words = topic_model.get_topic(tid)\n",
    "    topic_name = f\"主题{tid}：\" + \"_\".join([w for w, _ in topic_words[:3]])\n",
    "    result[topic_name] = [\"\".join([s for t, s in doc if t == tid]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. 保存 ==========\n",
    "output_path = \"/Users/yukun/Desktop/mda/客观分析预处理/客观分析/结构化主题输出.csv\"\n",
    "result.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ 分析完成，结果保存至：{output_path}\")# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "2025-06-07 20:46:01,621 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 145/145 [00:08<00:00, 17.30it/s]\n",
      "2025-06-07 20:46:14,858 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-07 20:46:14,860 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-07 20:46:16,996 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-07 20:46:16,998 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-07 20:46:17,097 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-07 20:46:17,105 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-07 20:46:17,233 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 分析完成，结果保存至：/Users/yukun/Desktop/mda/客观分析预处理/客观分析/结构化主题输出.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== 1. 读取所有 txt 文件 ==========\n",
    "root_path = \"/Users/yukun/Desktop/mda/MDA训练集\"\n",
    "file_list = sorted([f for f in os.listdir(root_path) if f.endswith('.txt')])\n",
    "documents = []\n",
    "\n",
    "for fname in file_list:\n",
    "    with open(os.path.join(root_path, fname), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        documents.append((fname, text))\n",
    "\n",
    "# ========== 2. 切句 ==========\n",
    "def split_sentences(text):\n",
    "    return [s.strip() for s in re.split(r\"[。！？\\n]\", text) if len(s.strip()) > 10]\n",
    "\n",
    "all_sentences = []\n",
    "doc_indices = []\n",
    "\n",
    "for idx, (fname, text) in enumerate(documents):\n",
    "    sents = split_sentences(text)\n",
    "    all_sentences.extend(sents)\n",
    "    doc_indices.extend([idx] * len(sents))  # 记录句子属于哪个文档\n",
    "\n",
    "# ========== 3. 加载 HuggingFace pipeline 作为 embedding ==========\n",
    "from transformers import pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class HFTransformerEmbedding(BaseEstimator):\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        embeddings = []\n",
    "        for doc in tqdm(documents, desc=\"Embedding Sentences\"):\n",
    "            features = self.pipe(doc, truncation=True, padding=True)\n",
    "            if isinstance(features, list):\n",
    "                features = np.mean(features[0], axis=0)\n",
    "            embeddings.append(features)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-zh\")\n",
    "embedding_model = HFTransformerEmbedding(pipe)\n",
    "\n",
    "# ========== 4. 建立 BERTopic 模型 ==========\n",
    "topic_model = BERTopic(embedding_model=embedding_model, language=\"chinese\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(all_sentences)\n",
    "\n",
    "# ========== 5. 每篇文档提取主题相关句子 ==========\n",
    "num_topics = min(10, len(set(topics)) - 1)  # -1是无关类\n",
    "docs = [[] for _ in range(len(documents))]  # 每个文档一组句子列表\n",
    "\n",
    "for tid in range(num_topics):\n",
    "    for sent, topic_id, doc_id in zip(all_sentences, topics, doc_indices):\n",
    "        if topic_id == tid:\n",
    "            docs[doc_id].append((tid, sent))\n",
    "\n",
    "# ========== 6. 构造结构化输出 DataFrame ==========\n",
    "result = pd.DataFrame()\n",
    "result[\"文件名\"] = [x[0] for x in documents]\n",
    "result[\"原文摘要\"] = [x[1][:300] for x in documents]  # 原文截断显示\n",
    "\n",
    "# 每个主题输出一列\n",
    "for tid in range(num_topics):\n",
    "    topic_words = topic_model.get_topic(tid)\n",
    "    topic_name = f\"主题{tid}：\" + \"_\".join([w for w, _ in topic_words[:3]])\n",
    "    result[topic_name] = [\"\".join([s for t, s in doc if t == tid]) for doc in docs]\n",
    "\n",
    "# ========== 7. 保存 ==========\n",
    "output_path = \"/Users/yukun/Desktop/mda/客观分析预处理/客观分析/结构化主题输出.csv\"\n",
    "result.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ 分析完成，结果保存至：{output_path}\")# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-zh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdatopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
