{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukun/Desktop/mda/mdapaper/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本列表（你应替换为你真实的 MDA 文本数据）\n",
    "mda_texts = [\n",
    "    \"公司2023年度实现营业收入增长，主要受益于主业扩张...\",\n",
    "    \"2022年本公司在战略布局方面进行了调整，提升风险管理...\",\n",
    "    \"受外部政策影响，公司整体毛利率出现下滑...\"\n",
    "]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# 构建 HuggingFace 数据集\n",
    "dataset = Dataset.from_dict({'text': mda_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 12.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 应用分词函数\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # 随机遮盖 15% token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件的列名： Index(['文件名', '董事会成员变动', '董秘变更', '高级管理层变更', '重大资产重组', '控股股东/实际控制人变更',\n",
      "       '子公司或重要分支机构重大变化', '对财务状况的分析', '营业收入增长情况', '净利润变动分析', '毛利率变动分析',\n",
      "       '销售费用率变化', '管理费用控制情况', '经营性现金流变化趋势', '应收账款变化与风险', '资产负债结构调整',\n",
      "       '利润质量分析（是否依赖非经常性损益）', '核心竞争力分析（品牌、成本、效率等）', '市场拓展与业务布局进展',\n",
      "       '区域/产品结构优化情况', '供应链/渠道建设进展', '战略合作与外部联盟情况', '投资项目执行及回报分析',\n",
      "       '企业文化建设与团队稳定性', '对标管理与精细化运营成果', '技术创新与研发投入', '新产品/新技术推出情况', '研发费用占比变化',\n",
      "       '专利数量与知识产权布局', '智能制造/数字化转型进展', '行业竞争格局分析', '行业发展趋势与机会识别', '国家政策影响与公司应对',\n",
      "       '未来经营计划与增长目标', '订单/客户/签约情况展望', '海外市场/出口业务计划', '对经济周期或宏观环境的判断',\n",
      "       '面临的主要风险因素及变化', '政策/监管风险与公司响应', '环保合规/碳排放压力', '土地/原材料/人工成本波动风险',\n",
      "       '资金链紧张预警', '核心客户集中度及流失风险', '可持续发展战略与目标', '社会责任履行与公益活动', '节能降耗/绿色生产措施'],\n",
      "      dtype='object')\n",
      "合并后的前几行数据：\n",
      "0    2429-兆驰股份-2021.txt 未提及 未提及 未提及 2021年11月出售兆驰供应链...\n",
      "1    895-双汇发展-2021.txt 未提及 未提及 未提及 未提及 未提及 未提及 1. 营...\n",
      "2    2475-立讯精密-2021.txt 未提及 未提及 未提及 未提及 未提及 立铠精密科技（...\n",
      "3    600566-济川药业-2021.txt 未提及 未提及 未提及 未提及 未提及 未提及 1...\n",
      "4    963-华东医药-2021.txt 未提及 未提及 未提及 收购浙江道尔生物75%股权、安徽...\n",
      "Name: combined_text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukun/Desktop/mda/mdapaper/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fed611790dc4e26a9777be9a6afecf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e227257d303a4e988711b2436a409eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`combined_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:759\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[32m    762\u001b[39m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[32m    763\u001b[39m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[32m    764\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:721\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[39m\u001b[34m(value, dtype)\u001b[39m\n\u001b[32m    720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(np.array(value))\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     65\u001b[39m trainer = Trainer(\n\u001b[32m     66\u001b[39m     model=model,\n\u001b[32m     67\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     data_collator=data_collator\n\u001b[32m     71\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/trainer.py:1859\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1857\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/trainer.py:2165\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2162\u001b[39m     rng_to_sync = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2164\u001b[39m step = -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2165\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   2168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/accelerate/data_loader.py:566\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/data/data_collator.py:45\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/data/data_collator.py:761\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    759\u001b[39m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m         batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    765\u001b[39m         batch = {\n\u001b[32m    766\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m    767\u001b[39m         }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     69\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3355\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3352\u001b[39m             batch_outputs[key] = []\n\u001b[32m   3353\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m-> \u001b[39m\u001b[32m3355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:224\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    220\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    222\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mda/mdapaper/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:775\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    771\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    772\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    773\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    774\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    776\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    777\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtruncation=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    778\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    779\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m expected).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    780\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`combined_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# 设置 CSV 文件路径\n",
    "csv_file_path = \"/Users/yukun/Desktop/mda/主观分析预处理/主观分析预处理结果/519GLM-Z1-9B-0414.env.csv\"  # 替换为你的 CSV 文件路径\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise FileNotFoundError(f\"文件 {csv_file_path} 不存在，请检查路径是否正确！\")\n",
    "\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 打印列名\n",
    "print(\"CSV 文件的列名：\", df.columns)\n",
    "\n",
    "# 将每一列的内容合并为一个整体的文本数据\n",
    "# 假设每一行的每一列都是一个文本片段，我们将它们拼接起来\n",
    "def combine_columns(row):\n",
    "    return \" \".join([str(cell) for cell in row])\n",
    "\n",
    "# 应用 combine_columns 函数到每一行\n",
    "df[\"combined_text\"] = df.apply(combine_columns, axis=1)\n",
    "\n",
    "# 打印合并后的前几行数据\n",
    "print(\"合并后的前几行数据：\")\n",
    "print(df[\"combined_text\"].head())\n",
    "\n",
    "# 将数据转换为 Hugging Face Dataset 格式\n",
    "dataset = Dataset.from_pandas(df[[\"combined_text\"]])  # 只保留合并后的文本列\n",
    "\n",
    "# 加载预训练的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"combined_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 应用分词函数到数据集\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 定义数据组合器\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mda-mlm\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# 加载预训练模型\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件中的前几行数据：\n",
      "                    文件名 董事会成员变动 董秘变更 高级管理层变更  \\\n",
      "0    2429-兆驰股份-2021.txt     未提及  未提及     未提及   \n",
      "1     895-双汇发展-2021.txt     未提及  未提及     未提及   \n",
      "2    2475-立讯精密-2021.txt     未提及  未提及     未提及   \n",
      "3  600566-济川药业-2021.txt     未提及  未提及     未提及   \n",
      "4     963-华东医药-2021.txt     未提及  未提及     未提及   \n",
      "\n",
      "                                              重大资产重组 控股股东/实际控制人变更  \\\n",
      "0  2021年11月出售兆驰供应链85.22%股权（交易价300万元），导致资产减值准备计提18...          未提及   \n",
      "1                                                未提及          未提及   \n",
      "2                                                未提及          未提及   \n",
      "3                                                未提及          未提及   \n",
      "4           收购浙江道尔生物75%股权、安徽华昌高科100%股权；成立湖北美琪健康合资公司。          未提及   \n",
      "\n",
      "                                      子公司或重要分支机构重大变化  \\\n",
      "0  2021年新增深圳市兆驰晶显技术、东莞兆驰智能、深圳市兆驰数码软件三家子公司，均未对业绩产生...   \n",
      "1                                                未提及   \n",
      "2  立铠精密科技（盐城）有限公司被收购并纳入合并范围，负责智能可穿戴设备生产；与奇瑞集团合资成立...   \n",
      "3                                                未提及   \n",
      "4  成立湖北美琪健康、成都海狸互联网医院；收购西班牙HighTech、美国R2、瑞士Kylane...   \n",
      "\n",
      "                                            对财务状况的分析  \\\n",
      "0  - 经营性现金流净额同比增23.27%，主要因销售收入增长及应收账款回款改善；  \\n  -...   \n",
      "1  1. 营业收入同比下降9.72%，主要受生猪价格下跌影响（生鲜产品降价超30%，影响收入下降...   \n",
      "2  - 营业收入增长66.43%（主要来自消费电子业务及立铠精密贡献）；  \\n  - 净利润同...   \n",
      "3  1. 经营性现金流净额同比增长6.32%，主要因销售回款改善；  \\n  2. 投资活动现金...   \n",
      "4  2021年营收345.63亿元（+2.61%），净利润23.02亿元（-18.38%），RO...   \n",
      "\n",
      "                                            营业收入增长情况  \\\n",
      "0         2021年营收225.38亿元，同比增长11.65%，近五年复合增长率24.69%。   \n",
      "1  - 生鲜产品收入同比下降19.05%（占营收58.60%），包装肉制品收入同比下降2.66%...   \n",
      "2  - 消费电子业务收入同比增长64.56%（占比87.46%）；  \\n  - 汽车业务收入同...   \n",
      "3  - 2021年营业收入763,051万元，同比增长23.77%；  \\n  - 增长驱动：疫...   \n",
      "4  医药工业收入101.09亿元（-8.43%），医药商业231.15亿元（+5.94%），医美...   \n",
      "\n",
      "                                             净利润变动分析  ...  \\\n",
      "0  2021年归母净利润3.33亿元，同比降72.63%，主要因计提18.93亿元资产减值准备（...  ...   \n",
      "1  - 归母净利润同比下降22.2%（主要因冻品业务亏损扩大）；  \\n  - 资产减值损失计提...  ...   \n",
      "2             净利润同比减少2.14%，主要因立铠精密高成本投入及智能穿戴业务阶段性回调。  ...   \n",
      "3  - 归母净利润171,917.59万元，同比增长34.60%；  \\n  - 主要因成本控制...  ...   \n",
      "4             扣非净利润21.89亿元（-9.91%），主要受集采降价及研发投入增加影响。  ...   \n",
      "\n",
      "                                       对经济周期或宏观环境的判断  \\\n",
      "0  - 2022年全球经济复苏乏力，但Mini/MicroLED技术渗透率提升（预计年复合增长率...   \n",
      "1  - 猪价处于下行周期（2021年生猪价格同比降43.9%），预计2022年维持低位；  \\n...   \n",
      "2              消费电子需求受全球经济波动影响，但汽车智能化、数据中心国产化提供长期支撑。   \n",
      "3  - 医药行业需求刚性，无显著周期性；  \\n  - 疫情后恢复性增长（2021年医药制造业利...   \n",
      "4                              未明确提及，但提到“逆水行舟”需创新转型。   \n",
      "\n",
      "                                        面临的主要风险因素及变化  \\\n",
      "0  - 供应链风险：LED芯片原材料（蓝宝石衬底）受国际垄断，2021年衬底自供率提升至30%；...   \n",
      "1                                                未提及   \n",
      "2  - 供应链风险：芯片短缺导致智能穿戴出货延迟；  \\n  - 客户集中风险：前五大客户占比8...   \n",
      "3  - 政策风险（如蒲地蓝消炎口服液退出3个省级医保目录）；  \\n  - 竞争加剧（带量采购、...   \n",
      "4  - 集采降价：核心产品毛利率承压；  \\n  - 研发风险：40+在研项目（5项III期临床...   \n",
      "\n",
      "                                        政策/监管风险与公司响应  \\\n",
      "0  - 受欧盟《电池新规》影响，2022年新增植物照明、车用LED等合规产品线；  \\n  - ...   \n",
      "1                                                未提及   \n",
      "2  - 环保合规：2021年投入2.3亿元升级绿色生产线；  \\n  - 数据安全：通过ISO ...   \n",
      "3  - 带量采购应对：优化成本结构（原料药自研、工艺改进）；  \\n  - 中药配方颗粒：紧跟江...   \n",
      "4  - 医美：严格筛选合作机构，推动产品注册（如GlacialRx）；  \\n  - 药品：加速...   \n",
      "\n",
      "                                          环保合规/碳排放压力  \\\n",
      "0  - 2021年LED芯片能耗降低18%，通过ISO14001认证；  \\n  - 2022年...   \n",
      "1                                                未提及   \n",
      "2                      2021年碳排放强度下降12%，计划2025年实现碳中和。   \n",
      "3                          - 未提及，但生产环节通过GMP升级强化质量管理。   \n",
      "4                                                未提及   \n",
      "\n",
      "                                     土地/原材料/人工成本波动风险  \\\n",
      "0                            - 蓝宝石衬底外购成本占比下降至25%（202   \n",
      "1                                                未提及   \n",
      "2              原材料成本上涨（+18.5%），但通过自动化降低人工成本（降幅达25%）。   \n",
      "3  - 中药材采购采用招标模式（如蒲公英、板蓝根），降低价格波动影响；  \\n  - 人工成本同...   \n",
      "4                                                未提及   \n",
      "\n",
      "                                     资金链紧张预警  \\\n",
      "0                                        未提及   \n",
      "1                                        未提及   \n",
      "2      经营活动现金流净额7.28亿元，短期偿债能力充足（货币资金14.2亿元）。   \n",
      "3  - 经营性现金流1,894亿元，支撑研发投入（523亿元）及产能扩张（杨凌基地）。   \n",
      "4               净现金流31.7亿元，短期借款1.24亿元，资金链稳健。   \n",
      "\n",
      "                         核心客户集中度及流失风险                      可持续发展战略与目标  \\\n",
      "0                                 未提及                             未提及   \n",
      "1                                 未提及                             未提及   \n",
      "2    前五大客户占比83.41%，与苹果、华为等签订5年以上合作协议。  2025年实现100%可再生能源供电，产品碳足迹降低30%。   \n",
      "3        - 前五大客户销售额占比5.66%，无单一客户依赖风险。  - 未提及具体战略，但研发投入占比7.14%，高于行业平均。   \n",
      "4  前五大客户占比19.16%，医药商业与90%跨国药企合作，风险可控。      ESG百强企业，计划2025年碳排放强度下降20%。   \n",
      "\n",
      "                             社会责任履行与公益活动  \\\n",
      "0                                    未提及   \n",
      "1                                    未提及   \n",
      "2  2021年捐赠1.2亿元用于教育、环保项目，员工持股计划覆盖2,000人。   \n",
      "3            - 未提及，但企业品牌获“全国先进基层党组织”等荣誉。   \n",
      "4                                    未提及   \n",
      "\n",
      "                                         节能降耗/绿色生产措施  \n",
      "0                                                未提及  \n",
      "1                                                未提及  \n",
      "2  - 智能制造：AI算法优化能耗，单位产值能耗下降8%；  \\n  - 循环经济：废料回收率提...  \n",
      "3                - 未提及，但生产成本通过工艺优化下降（如制造费用同比-9.66%）。  \n",
      "4  未提及  \\n\\n（注：部分条目因原文未明确数据或策略，标注“未提及”或需结合上下文推断，如...  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CSV 文件中没有找到 'text' 列，请检查列名是否正确！",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 确保 'text' 列存在\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCSV 文件中没有找到 \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m 列，请检查列名是否正确！\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 将数据转换为 Hugging Face Dataset 格式\u001b[39;00m\n\u001b[32m     26\u001b[39m dataset = Dataset.from_pandas(df)\n",
      "\u001b[31mValueError\u001b[39m: CSV 文件中没有找到 'text' 列，请检查列名是否正确！"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# 设置 CSV 文件路径\n",
    "csv_file_path = \"/Users/yukun/Desktop/mda/主观分析预处理/主观分析预处理结果/519GLM-Z1-9B-0414.env.csv\"  # 替换为你的 CSV 文件路径\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise FileNotFoundError(f\"文件 {csv_file_path} 不存在，请检查路径是否正确！\")\n",
    "\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 检查数据\n",
    "print(\"CSV 文件中的前几行数据：\")\n",
    "print(df.head())\n",
    "\n",
    "# 确保 'text' 列存在\n",
    "if \"text\" not in df.columns:\n",
    "    raise ValueError(\"CSV 文件中没有找到 'text' 列，请检查列名是否正确！\")\n",
    "\n",
    "# 将数据转换为 Hugging Face Dataset 格式\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 加载预训练的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 应用分词函数到数据集\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 定义数据组合器\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mda-mlm\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# 加载预训练模型\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdapaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
