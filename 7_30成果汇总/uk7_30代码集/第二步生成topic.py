# -*- coding: utf-8 -*-
import os
import re
import pandas as pd
from tqdm import tqdm
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from collections import Counter
import jieba
import jieba.analyse

# ==== è·¯å¾„é…ç½® ====
base_input_path = "/Users/yukun/Desktop/mda/è¡Œä¸š"        # è¾“å…¥ï¼šå„è¡Œä¸šæŒ‰å¹´å½’æ¡£çš„æ–‡æœ¬ç›®å½•
base_output_path = "/Users/yukun/Desktop/mda/topic"      # è¾“å‡ºï¼šè¡Œä¸šä¸»é¢˜åˆ†æç»“æœä¿å­˜ç›®å½•
pointer_file = os.path.join(base_output_path, "progress_pointer.txt")  # è¿›åº¦æŒ‡é’ˆæ–‡ä»¶ï¼Œæ–­ç‚¹ç»­è·‘ç”¨
os.makedirs(base_output_path, exist_ok=True)             # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

# ==== è‡ªåŠ¨è¡¥å…¨æŒ‡é’ˆï¼Œé˜²æ­¢é‡å¤åˆ†æ ====
existing_keys = set()  # ç”¨äºè®°å½•å·²è¾“å‡ºçš„è¡Œä¸š+å¹´ä»½ç»„åˆ
for industry_dir in os.listdir(base_output_path):
    output_industry_path = os.path.join(base_output_path, industry_dir)
    if not os.path.isdir(output_industry_path):
        continue
    for fname in os.listdir(output_industry_path):
        # è¯†åˆ«å½¢å¦‚â€œè¡Œä¸šå2022topic.xlsxâ€çš„è¾“å‡ºæ–‡ä»¶
        match = re.match(r"^(.*?)(\d{4})topic\.xlsx$", fname)
        if match:
            industry_name = match.group(1)
            year = match.group(2)
            existing_keys.add(f"{industry_name}{year}")

# ==== åˆå¹¶æŒ‡é’ˆæ–‡ä»¶ä¸­çš„å†å²å·²å¤„ç†key ====
if os.path.exists(pointer_file):
    with open(pointer_file, "r") as f:
        old_keys = set(line.strip() for line in f if line.strip())
else:
    old_keys = set()
new_keys = existing_keys | old_keys  # åˆå¹¶å†å²ä¸æœ€æ–°keyï¼Œä¿è¯æ–­ç‚¹ç»­è·‘
with open(pointer_file, "w") as f:   # æŒä¹…åŒ–è¿›åº¦æŒ‡é’ˆ
    for k in sorted(new_keys):
        f.write(k + "\n")
done_keys = new_keys                 # ç”¨äºåç»­è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡

# ==== åŠ è½½ä¸­æ–‡å¥åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºBERTopicï¼‰ ====
embedding_model = SentenceTransformer("BAAI/bge-small-zh")

# ==== å®šä¹‰é€šç”¨ä¸»é¢˜åŠå…³é”®è¯ï¼Œä¾¿äºè‡ªåŠ¨æ ‡ç­¾å½’ç±» ====
common_topics = {
    "æœªæ¥ç ”å‘": ["ç ”å‘", "åˆ›æ–°", "æŠ€æœ¯å‡çº§", "æŠ€æœ¯æŠ•å…¥", "æŠ€æœ¯åˆ›æ–°"],
    "é£é™©å› ç´ ": ["é£é™©", "ä¸ç¡®å®š", "æŒ‘æˆ˜", "éšæ‚£", "æ³¢åŠ¨"],
    "å…¬å¸æˆ˜ç•¥": ["æˆ˜ç•¥", "ç›®æ ‡", "å¸ƒå±€", "æ–¹å‘", "è“å›¾"],
    "å¸‚åœºè¶‹åŠ¿": ["å¸‚åœº", "éœ€æ±‚", "å®¢æˆ·", "ç«äº‰", "å æœ‰ç‡"],
    "è´¢åŠ¡çŠ¶å†µ": ["æ”¶å…¥", "åˆ©æ¶¦", "æ¯›åˆ©", "è¥æ”¶", "å¢é•¿"],
    "ç»è¥æˆæœ": ["ç»è¥", "å›æŠ¥", "ç»©æ•ˆ", "ä¸šç»©"],
    "æ”¿ç­–ç¯å¢ƒ": ["æ”¿ç­–", "ç›‘ç®¡", "æ³•è§„", "æ³•å¾‹", "åˆè§„"]
}

# ==== éå†æ‰€æœ‰è¡Œä¸šæ–‡ä»¶å¤¹ ====
for industry_dir in os.listdir(base_input_path):
    industry_path = os.path.join(base_input_path, industry_dir)
    if not os.path.isdir(industry_path):
        continue
    if industry_dir in ["topic", "rank_andoutput"]:  # è·³è¿‡ç‰¹å®šç›®å½•
        continue

    # è§£æè¡Œä¸šåç§°ï¼ˆè‹¥æœ‰â€œè¡Œä¸šå+æ•°å­—â€å‘½åï¼Œåˆ™åªå–è¡Œä¸šåéƒ¨åˆ†ï¼‰
    match = re.match(r'^(.+?)(\d+)$', industry_dir)
    industry_name = match.group(1) if match else industry_dir
    output_industry_path = os.path.join(base_output_path, industry_name)
    os.makedirs(output_industry_path, exist_ok=True)

    # ==== éå†è¯¥è¡Œä¸šä¸‹çš„æ‰€æœ‰å¹´ä»½å­æ–‡ä»¶å¤¹ ====
    for year_dir in sorted(os.listdir(industry_path)):
        year_path = os.path.join(industry_path, year_dir)
        # åªå¤„ç†4ä½æ•°å­—çš„å¹´ä»½ç›®å½•
        if not os.path.isdir(year_path) or not re.match(r'^\d{4}$', year_dir):
            continue

        industry_year_key = f"{industry_name}{year_dir}"
        if industry_year_key in done_keys:
            print(f"å·²å¤„ç†ï¼Œè·³è¿‡ï¼š{industry_year_key}")
            continue

        try:
            # ==== æ”¶é›†æ‰€æœ‰txtæ–‡ä»¶ ====
            file_list = sorted([f for f in os.listdir(year_path) if f.endswith('.txt')])
            if not file_list:
                continue

            # === æ–‡æ¡£ä¸å¥å­åˆ†å‰² ===
            documents, all_sentences, doc_indices = [], [], []
            for fname in file_list:
                file_path = os.path.join(year_path, fname)
                text = None
                # å…¼å®¹å¤šç§å¸¸è§ç¼–ç ï¼ˆé˜²æ­¢ä¹±ç /æŠ¥é”™ï¼‰
                for enc in ['utf-8', 'gbk', 'latin1', 'utf-16', 'big5']:
                    try:
                        with open(file_path, 'r', encoding=enc) as f:
                            text = f.read()
                        break
                    except Exception:
                        continue
                if not text:
                    print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ï¼ˆæ— æ³•è¯»å–ç¼–ç ï¼‰: {file_path}")
                    continue
                doc_id = len(documents)
                documents.append((fname, text))
                # æŒ‰â€œã€‚ï¼ï¼Ÿ\nâ€åˆ‡å¥ï¼Œå¥é•¿å¤§äº10å­—ç¬¦æ‰çº³å…¥
                for s in re.split(r"[ã€‚ï¼ï¼Ÿ\n]", text):
                    s = s.strip()
                    if len(s) > 10:
                        all_sentences.append(s)
                        doc_indices.append(doc_id)

            if not documents or not all_sentences:
                continue

            # ==== æå–è¡Œä¸šå…³é”®è¯ï¼ˆç”¨äºä¸»é¢˜äºŒæ¬¡å½’ç±»ï¼‰====
            combined_text = " ".join(doc_text for _, doc_text in documents)
            candidate_keywords = jieba.analyse.extract_tags(combined_text, topK=50)
            industry_keywords = candidate_keywords.copy()
            if industry_name and industry_name not in industry_keywords:
                industry_keywords.insert(0, industry_name)

            # ==== é€šç”¨ä¸»é¢˜å½’ç±» ====
            # å»ºç«‹æ¯ä¸ªæ–‡æ¡£çš„é€šç”¨ä¸»é¢˜å¥å­åˆ—è¡¨
            common_assignments = [[] for _ in range(len(documents))]
            assigned_idx = set()  # å·²ç»è¢«åˆ†é…é€šç”¨ä¸»é¢˜çš„å¥å­ç´¢å¼•
            for topic, keywords in common_topics.items():
                for i, sent in enumerate(all_sentences):
                    if i in assigned_idx:
                        continue
                    if any(kw in sent for kw in keywords):
                        doc_id = doc_indices[i]
                        if doc_id >= len(common_assignments):
                            continue
                        common_assignments[doc_id].append((topic, sent))
                        assigned_idx.add(i)

            # ==== è¡Œä¸šä¸»é¢˜å½’ç±»ï¼ˆç”¨BERTopicåšæœªå½’ç±»éƒ¨åˆ†èšç±»ï¼‰ ====
            # åªå¯¹æœªå½’ç±»çš„å¥å­åšèšç±»
            unassigned_sentences = [s for idx, s in enumerate(all_sentences) if idx not in assigned_idx]
            unassigned_indices = [doc_indices[idx] for idx in range(len(all_sentences)) if idx not in assigned_idx]

            if unassigned_sentences and len(unassigned_sentences) > 10:
                # èšç±»ä¸»é¢˜æ•°æœ€å¤šä¸ºå¥å­æ•°æˆ–6ï¼ˆäºŒè€…å–å°ï¼‰
                topic_model = BERTopic(
                    embedding_model=embedding_model,
                    language="chinese",
                    nr_topics=min(len(unassigned_sentences), 6)
                )
                topics, _ = topic_model.fit_transform(unassigned_sentences)

                # ç»Ÿè®¡æ¯ä¸ªèšç±»ä¸»é¢˜å‡ºç°çš„è¡Œä¸šå…³é”®è¯æ¬¡æ•°
                theme2indices = {}
                for j, topic_id in enumerate(topics):
                    theme2indices.setdefault(topic_id, []).append(j)
                theme_scores = {}
                theme_topword = {}
                for tid, idx_list in theme2indices.items():
                    cnt = Counter()
                    for j in idx_list:
                        sent = unassigned_sentences[j]
                        for kw in industry_keywords:
                            if kw in sent:
                                cnt[kw] += 1
                    theme_scores[tid] = sum(cnt.values())
                    theme_topword[tid] = cnt.most_common(1)[0][0] if cnt else None
                # é€‰è¡Œä¸šä¸»é¢˜
                if theme_scores:
                    top_theme_id = max(theme_scores.items(), key=lambda x: x[1])[0]
                else:
                    top_theme_id = max(theme2indices.items(), key=lambda x: len(x[1]))[0]
                main_keyword = theme_topword.get(top_theme_id) or "è¡Œä¸šä¸»é¢˜"
                industry_column_name = f"è¡Œä¸šä¸»é¢˜ï¼š{main_keyword}"
            else:
                top_theme_id = None
                industry_column_name = "è¡Œä¸šä¸»é¢˜"

            # ==== æ±‡æ€»è¾“å‡ºDataFrame ====
            result = pd.DataFrame()
            # æ–‡ä»¶ååˆ—
            result["æ–‡ä»¶å"] = [os.path.splitext(name)[0] for name, _ in documents]
            # é€šç”¨ä¸»é¢˜åˆ—
            for topic in common_topics:
                result[topic] = [
                    " ".join([sent for t, sent in common_assignments[file_idx] if t == topic])
                    for file_idx in range(len(documents))
                ]
            # è¡Œä¸šä¸»é¢˜åˆ—
            industry_column_data = []
            if top_theme_id is not None:
                for file_idx in range(len(documents)):
                    sentences_for_file = []
                    for j, sent in enumerate(unassigned_sentences):
                        if topics[j] == top_theme_id and unassigned_indices[j] == file_idx:
                            if any(kw in sent for kw in industry_keywords):
                                sentences_for_file.append(sent)
                    # åªå–å”¯ä¸€å¥å­ï¼Œä¸”æœ€å¤š5å¥ï¼Œæ‹¼æˆä¸€ä¸ªå­—ç¬¦ä¸²
                    unique_sents = []
                    for s in sentences_for_file:
                        if s not in unique_sents:
                            unique_sents.append(s)
                    industry_column_data.append(" ".join(unique_sents[:5]))
            else:
                industry_column_data = [""] * len(documents)
            result[industry_column_name] = industry_column_data

            # ==== æ¸…æ´—éæ³•æ§åˆ¶å­—ç¬¦ï¼Œé¿å…Excelè¯»å–æŠ¥é”™ ====
            def clean_excel_string(x):
                if not isinstance(x, str):
                    x = str(x) if x is not None else ""
                return re.sub(r'[\x00-\x1F\x7F-\x9F]', '', x)
            result = result.applymap(clean_excel_string)

            # ==== ä¿å­˜ç»“æœ ====
            output_file = os.path.join(output_industry_path, f"{industry_name}{year_dir}topic.xlsx")
            result.to_excel(output_file, index=False)
            print(f"âœ… å·²ä¿å­˜: {output_file}")

            # ==== æŒ‡é’ˆæ›´æ–°ï¼Œé˜²æ­¢é‡å¤å¤„ç† ====
            with open(pointer_file, "a") as f:
                f.write(industry_year_key + "\n")
            done_keys.add(industry_year_key)

        except Exception as e:
            print(f"âŒ å¤„ç† [{industry_name} {year_dir}] æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
            continue  # å…³é”®ï¼šå¼‚å¸¸ä¹Ÿä¸å½±å“åç»­è¡Œä¸š/å¹´ä»½å¤„ç†

print("ğŸ å…¨éƒ¨ä»»åŠ¡æ‰§è¡Œå®Œæ¯•")